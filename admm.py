# -*- coding:utf-8 -*-
# 
# Author: 
# Time: 

import torch
import fnmatch
import numpy as np
import os

#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
#  äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ ä¼˜åŒ–ç®—æ³•ï¼Œé€šå¸¸ç”¨äºè§£å†³å¸¦æœ‰çº¦æŸå’Œæ­£åˆ™åŒ–çš„ä¼˜åŒ–é—®é¢˜ã€‚
class ADMM:
    def __init__(self, gsmodel, rho, device):
        self.gsmodel = gsmodel 
        self.device = device # æŒ‡å®šå¼ é‡å­˜å‚¨çš„ä½ç½®ï¼ˆCPU æˆ– GPUï¼‰
        self.init_rho = rho # æ ‡é‡ï¼Œç”¨äº ADMM ä¸­çš„æƒ©ç½šé¡¹
        # uå’Œzæ˜¯ADMMç®—æ³•ä¸­çš„è¾…åŠ©å˜é‡ï¼ˆå¯¹å¶å˜é‡ï¼‰
        self.u = {}
        self.z = {}
        self.rho = self.init_rho
        opacity = self.gsmodel.get_opacity
        self.u = torch.zeros(opacity.shape).to(device)
        self.z = torch.Tensor(opacity.data.cpu().clone().detach()).to(device)

    # æ³¨æ„,paperä¸­çš„aå°±æ˜¯è¿™é‡Œçš„opacity
    def update(self, threshold, update_u = True):
        #  a + Î»
        z = self.gsmodel.get_opacity + self.u
        # z â† proxh(a + Î»)  è£å‰ªè¾…åŠ©å˜é‡z  Update z via Eq. 16;  è¿™é‡Œç›¸å½“äºæ˜¯h 
        self.z = torch.Tensor(self.prune_z(z, threshold)).to(self.device)
        # Î» = Î» + a âˆ’ z.  Update Î» via Eq. 17
        if update_u: 
            with torch.no_grad():
                diff =  self.gsmodel.get_opacity - self.z
                self.u += diff

    #  è¯¥æ–¹æ³•æ ¹æ®ä¸åŒçš„ç­–ç•¥ï¼ˆç”± opt å‚æ•°æ§åˆ¶ï¼‰æ¥æ›´æ–° zï¼š
    def prune_z(self, z, threshold):
        z_update = self.metrics_sort(z, threshold)  
        return z_update

    # opacity å’Œ zä¹‹é—´çš„å·®å¼‚å°±æ˜¯æŸå¤±,zæ˜¯è¾…åŠ©ä¸ç”¨å…³æ³¨
    def get_admm_loss(self, loss): 
        return 0.5 * self.rho * (torch.norm(self.gsmodel.get_opacity - self.z + self.u, p=2)) ** 2

    def adjust_rho(self, epoch, epochs, factor=5): # æ ¹æ®è®­ç»ƒçš„å½“å‰è¿›åº¦ï¼ˆepoch å’Œ epochsï¼‰è°ƒæ•´ rho å€¼ã€‚é€šå¸¸ï¼Œrho ä¼šéšç€è®­ç»ƒçš„è¿›å±•è€Œå¢å¤§ï¼Œä»è€Œå¢åŠ å¯¹çº¦æŸçš„æƒ©ç½š
        if epoch > int(0.85 * epochs):
            self.rho = factor * self.init_rho
    
    def metrics_sort(self, z, threshold): # æ ¹æ®é€æ˜åº¦çš„æ’åºå€¼æ¥æ›´æ–° zã€‚å®ƒé€šè¿‡å°†é€æ˜åº¦æŒ‰å‡åºæ’åºå¹¶åº”ç”¨ä¸€ä¸ªé˜ˆå€¼æ¥é€‰æ‹©é€æ˜åº¦å€¼
        index = int(threshold * len(z))
        z_sort = {}
        z_update = torch.zeros(z.shape)
        z_sort, _ = torch.sort(z, 0)
        z_threshold = z_sort[index-1]
        z_update= ((z > z_threshold) * z)  
        return z_update
    
    def metrics_sample(self, z, opt): # è¯¥æ–¹æ³•æ ¹æ®é€æ˜åº¦å€¼çš„ç›¸å¯¹æƒé‡è¿›è¡Œéšæœºé‡‡æ ·ã€‚é¦–å…ˆï¼Œå°†é€æ˜åº¦å€¼å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶æŒ‰æ­¤åˆ†å¸ƒéšæœºé€‰æ‹©æ ·æœ¬ã€‚
        index = int((1 - opt.pruning_threshold) * len(z))
        prob = z / torch.sum(z)
        prob = prob.reshape(-1).cpu().numpy()
        indices = torch.tensor(np.random.choice(len(z), index, p = prob, replace=False))
        expand_indices = torch.zeros(z.shape[0] - len(indices)).int()
        indices = torch.cat((indices, expand_indices),0).to(self.device)
        z_update = torch.zeros(z.shape).to(self.device)
        z_update[indices] = z[indices]
        return z_update

    def metrics_imp_score(self, z, imp_score, opt): # è¯¥æ–¹æ³•åŸºäºé‡è¦æ€§åˆ†æ•°ï¼ˆimp_scoreï¼‰æ›´æ–° zã€‚é‡è¦æ€§åˆ†æ•°ä½äºæŸä¸ªé˜ˆå€¼çš„é€æ˜åº¦å€¼ä¼šè¢«ç½®ä¸º 0ï¼Œä»è€Œå¯¹é‡è¦æ€§è¾ƒä½çš„éƒ¨åˆ†è¿›è¡Œç¨€ç–åŒ–ã€‚
        index = int(opt.pruning_threshold * len(z))
        imp_score_sort = {}
        imp_score_sort, _ = torch.sort(imp_score, 0)
        imp_score_threshold = imp_score_sort[index-1]
        indices = imp_score < imp_score_threshold 
        z[indices == 1] = 0  
        return z        



def get_unactivate_opacity(gaussians):
    opacity = gaussians._opacity[:, 0]
    scores = opacity
    return scores

def get_pruning_mask(scores, threshold):        
    scores_sorted, _ = torch.sort(scores, 0)
    threshold_idx = int(threshold * len(scores_sorted))
    abs_threshold = scores_sorted[threshold_idx - 1]
    mask = (scores <= abs_threshold).squeeze()
    return mask

def check_grad_leakage(model, optimizer=None):
    """
    é€‚ç”¨äºé nn.Module çš„æ¨¡å‹ï¼Œè‡ªè¡Œéå†å±æ€§æ£€æŸ¥å‚æ•°æ¢¯åº¦çŠ¶æ€ã€‚
    """
    print("\n=== ğŸš¨ æ£€æŸ¥æ¢¯åº¦æ³„éœ²å’Œä¼˜åŒ–å™¨å‚æ•°ï¼ˆè‡ªå®šä¹‰æ¨¡å‹ï¼‰ ===")
    leak_found = False

    for name in dir(model):
        param = getattr(model, name)
        if isinstance(param, torch.Tensor) and param.requires_grad is not None:
            if param.grad is not None and not param.requires_grad:
                print(f"âš ï¸ å‚æ•° '{name}' æœ‰ gradï¼Œä½† requires_grad=Falseï¼å¯èƒ½æ³„éœ²ã€‚")
                leak_found = True
            elif param.requires_grad:
                print(f"âœ… å‚æ•° '{name}' è®¾ç½®ä¸ºå¯è®­ç»ƒ (requires_grad=True)ã€‚")
            else:
                print(f"ğŸ”’ å‚æ•° '{name}' ä¸å¯è®­ç»ƒ (requires_grad=False)ã€‚")

    # æ£€æŸ¥ optimizer å‚æ•°ç»„
    if optimizer is not None:
        print("\n=== ğŸ” æ£€æŸ¥ Optimizer ä¸­çš„å‚æ•° ===")
        for i, group in enumerate(optimizer.param_groups):
            for p in group['params']:
                if not p.requires_grad:
                    print(f"âš ï¸ Optimizer param_group[{i}] ä¸­åŒ…å« requires_grad=False çš„å‚æ•°")
                    leak_found = True

    if not leak_found:
        print("âœ… æ²¡å‘ç°æ³„éœ²æˆ–å¼‚å¸¸å‚æ•°ã€‚")
    raise("=== æ£€æŸ¥å®Œæˆ ===\n")
